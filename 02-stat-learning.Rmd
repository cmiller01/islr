---
author: "Christian"
date: "January 5, 2017"
output:
  pdf_document:
    toc: true
---

# Statistical Learning {#stat-learning}

## Notes

## Exercises

### Conceptual
__Question 1__

Indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

a) With a large sample size _n_ and a small number of predictors _p_ a flexible statistical learning method will perform better than inflexible because a flexible method will have enough training data and a large parameter space to search to create a good model

b) With a large number of predictors _p_ and a small number of observations _n_ an inflexible method will perform better because the solution space will be too sparse for a flexible method and it will likely over-fit. With an inflexible or parameterized method the solution space is constrained and it will likely create a more general model

c) With a relationship between predictors and response that is highly non-linear a more flexible method will perform better because an inflexible model can't accurately model non-linear relationships (usually)

d) With a high variance of the error terms a less flexible model will be better because a more flexible method may over-fit due to the high amount of error.

__Question 2__

Explain whether each scenario is a classification or regression problem, indicate whether we are most interested in inference or prediction and provide _n_ and _p_

a) For the CEO salary we are creating a regression problem and are interested in inference. _n_ = 500, _p_ = 3 (profit, # employees, industry)

b) With the product launch we are creating a classification problem where we are most interested in prediction. _n_ = 20, _p_ = 13

c) For the Forex change are creating a regression problem we are creating a regression problem where we are most interested in prediction. _n_ = 50, _p_ = 3

__Question 3__

Bias-variance decomposition

```{r ch2_conceptual_q3, message=FALSE}
library(tidyverse)
# plot bias, variance, training error, test error and Bayes/irreducible error curves going from less flexible to more flexible stat learning methods
# bias is high initially for inflexible with low var
# training error likely reduces assymptotically
# test error is U-shaped
# Bayes error is minimum dotted line
#df <- tibble(flexibility=seq(0,2*pi,0.001))
#df <- mutate(df,bias=1/flexibility+1/flexibility^(1/2),variance=flexibility^(1/2),training_error=1/flexibility + 1, test_error=1,irreducible_error=rep(1,100))


```

__Question 4__

Think of real-life applications for statistical learning, describe response & predictors, is your goal inference or prediction?

a) For classification:
  + Predicting who will purchase a product: response ~ conversions, predictors ~ demographics (age, sex, location) + behavior (clicks/views of site, usage of other products, etc.). Goal is inference to better target
  + Predicting who will click on an ad: response ~ conversions, predictors ~ similar to above, but goal is just prediction to better deliver advertising, additional features could be added even if they don't help intuition
  + Medical Diagnosis: response ~ disease/not disease, predictors ~ depending on domain, but could be demographics, behavior (e.g. diet, exercise), test results (e.g. cholesterol levels), goal is prediction as simply trying to deliver an accurate diagnosis

b) Ibid, regression
  + Forecasting sales of watermelons: response ~ sales, predictors ~ temperature, date (seasonality effects), price of product. Goal is inference to better understand consumer demand so product isn't wasted or short of supply and promotions can be planned.
  + Predicting tax liability for IRS: response ~ tax owed, predictors ~ features from 1040, other tax forms, history of prior tax filings. Goal is prediction, can regress predicted tax liability against actual to determine when/if to launch an audit
  + Predicting score of basketball game: response ~ final score, predictors ~ team features (wins, losses, margins, etc.), player info like injuries, starting lineup. Goal is prediction for purposes of gambling.

c) Cluster-analysis
  + Finding similar groups of people to those who are currently customers to target for marketing
  + Identifying characteristics of different pathologies, e.g. tumorous cells have a bunch of different gene markers, what other cells have similar markers?
  + Exploratory analysis to understand what features might be important/relevant in constructing a model
  
__Question 5__

What are the advantages and disadvantages of a very flexible (vs less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred?

A more flexible approach (often) has the benefit of being a better fitting model (ie less bias), meaning better predictions. However, there can be some disadvantages if that model becomes very complex and less generalizeable AND potentially it is prone to overfitting. A more flexible approach may be preferred when there is a greater emphasis on predictive performance vs model inference, when there is ample data (to prevent overfitting) and if the behavior being modeled is complex, so that a less flexible model wouldn't be a good representation.

__Question 6__

Describe the differences between a parametric and non-parametric statistical learning approach. What are the advantages/disadvantages (vs non-parametric)?

A parametric learning approach means that a functional form for a model is specified to describe the relationship, then parameters of that model are estimated given the training data. By contrast a non-parametric approach don't make any assumptions about the form/model of the relationship and try to estimate some function that fits the data as closely as possible. The benefits of a parametric approach are that the model is generally more interpretable, giving more inferential power, and that it may be more generalizeable; however parametric approaches may not be able to fit the data as well and cannot necessarily fit to more complex functions.

__Question 7__

The table below provides a training data set containting six observations, three predictors and one qualitative response variable.
```{r ch2_conceptual_q7}
tmp <- data.frame(x1=c(0,2,0,0,-1,1),x2=c(3,0,1,1,0,1),x3=c(0,0,3,2,1,1),y=c('Red','Red','Red','Green','Green','Red'))
tmp
```
Suppose we wish to use this data set to make a prediction for _Y_ when X1 = X2 = X3 = 0 using K-nearest neighbors.

a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0
```{r ch2_conceptual_q7_a}
tmp$distance <- ((tmp$x1 - 0)^2 + (tmp$x2 - 0)^2 + (tmp$x3 - 0)^2)^(1/2)
tmp
```
b) What is our prediction with K=1? Why?

Our prediction is `Green` because that is the single closest observation (distance of 1.414)

c) What is our prediction with K=3? Why?

Prediction of `Red` because our closes observations are 2, 5 and 6 which are `Red`, `Green` and `Red` thus by simple majority we go with `Red`

d) If the Bayes decision boundary in this problem is highly non-linear then would we expect the _best_ value for K to be large or small? Why?

Small K because the decision boundary needs to be very flexible to accomidate the non-linearity; a large K would produce a less flexible model that is unlikely to fit the complex data well.

### Applied

__Question 8__

Use `College` data set...walkthough questions/directions

```{r ch2_q7}
# part a load data
# previously downloaded file from http://www-bcf.usc.edu/~gareth/ISL/College.csv
college <- read.csv(file='datasets/College.csv')

# part b rename rows
# inspect data (not run in script)
# fix(college)

# change rownames
rownames(college) <- college[,1]

# drop the names column
college <- college[,-1]

# part c start summarizing
# i. summary
summary(college)

# ii. plot pairwise
pairs(college[,1:10])

# iii. boxplot Outstate vs Private
plot(college$Private,college$Outstate)

# iv. new qualitative variable
Elite <- rep("No",nrow(college))
Elite[college$Top10perc>50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college,Elite)

summary(Elite)

plot(college$Elite,college$Outstate)

# v. histograms
par(mfrow=c(2,2))

hist(college$F.Undergrad)
hist(college$Personal,breaks="Scott")
hist(college$Grad.Rate,breaks="FD")
hist(college$Outstate,breaks=10)

# vi. continue to explore the data...
# just using base functions described thus far
college$Acceptance.Rate <- college$Accept / college$Apps

college$Perc.ExpendOutstate <- college$Expend / college$Outstate

# elite stuff
par(mfrow=c(2,2))
plot(college$Elite,college$Acceptance.Rate)
plot(college$Elite,college$perc.alumni)
plot(college$Elite,college$Perc.ExpendOutstate)
plot(college$Elite,college$Grad.Rate)

```

Summary of further analysis:

  * Further exploration of "elite" colleges shows that they have a substantially lower acceptance rate (median of `r median(college[college$Elite=="Yes","Acceptance.Rate"])` vs `r median(college[college$Elite=="No","Acceptance.Rate"])` for non-elite).
  * Alumni of elite schools appear to be more likely to donate to their schools and they also have higher graduation rates.
  * Note there is an outlier in the graduation rate for `r row.names(college[(college$Grad.Rate>100),])` which had a graduation rate over 100%, which leads to some questions about how the data was generated.

__Question 9__

```{r ch2_q9}
# previously downloaded from: 
Auto <- read.csv(file='datasets/Auto.csv')
# TODO
```


__Question 10__

a) Rows/columns/describe data

`r library(MASS)`

The `Boston` dataset has `r nrow(Boston)` rows and `r ncol(Boston)` columns. Each row represents a town with the different columns indicating different conditions of the area like crime, values, density, location, etc.

b) Make some pairwise scatterplots and describe your findings

```{r ch2_q10_b}
pairs(Boston,upper.panel = NULL)
```

Observations:
  * There are a number of predictors that have no few suggestive relationships, for example the impact of the "charles river" dummy is mixed. `zn`, `indus`, `tax` and `ptratio` are also a bit ambiguous against most variables.
  * However, there are a number of potentially meaningful relationships here:
    * The `chas` variable is suggestive that crime is low when on the river (the converse is not necessarily the case, some towns have high crime, some low if NOT on the river)
    * Crime is higher overall as distance to employment centers decreases
    * High value home areas are generally low crime
    * There is a positive correlation between industrial percentage and nox concentration
    * Nox concentrations are also much higher as `dis` decreases, also it appears to be slightly negatively correlated with median home values
    * There's a strong negative relationship between `lstat` ie income and median value of the homes.
    
c) Predictors associated with crime rate?

  * As mentioned above the Charles River variable, distance from employment centers, as well as a negative relationship between crime and median home value
  
d) Suburbs of Boston with high crime rates/tax rates/etc. comment on range of each predictor

```{r ch2_q10_d}
apply(Boston,2,range)

apply(Boston,2,quantile,probs=c(0.1,0.25,0.5,0.75,0.9))
```
Most of these values have a broad range of values, however they are largely clumped around 


e) Suburbs bound the Charles River: `r sum(Boston$chas)`

f) Median pupil-teacher ratio: `r median
